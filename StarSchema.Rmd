---
title: "Some crude time series analysis experiments with the Bayer data"
author: "Andrew Lowe"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Introduction

In my free time, outside of office hours, I'm studying time series analysis. It would be fun to see what we can learn from the Bayer data, which is hierarchical time series data. We won't do anything too sophisticated at this stage, and we probably don't have enough data points anyway. All of this was developed while waiting for my KNIME workflows to finish running over the full 10 GB data (a couple of hours per workflow), so there was ample time to develop and document what follows. 

## Preparation

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We're going to need this stuff:
```{r message = FALSE}
require(tidyverse)
```

Read in the data -- we don't have enough RAM on this machine to read in the whole "StarSchema" data, so we'll just load the "Golden Standard" data, which is a subset of the full data and will be enough to get a flavour of what might be possible:

```{r message = FALSE}
dat <- read_csv("C:/Work/Projects/Bayer/ws/Tableau/_work/output/GoldenStandardAddressable.csv")
```

We need to impose some filtering criteria; this is identical to what is done in the KNIME workflows:
```{r}
# We're only going to consider the SKUs for which we have information on how to
# categorise them, and only pharmacy sales, ann we'll consider both sales with
# and without prescription:
include.rows <- with(dat,
                     (SOURCE_CODE == "OTC_PEC") & # Over The Counter sales
                       (MARKET_LEVEL == "TOTAL") & 
                       (IS_CATEGORY_MAPPED == "YES" | IS_CATEGORY_MAPPED == "Y") &
                       (
                         (Distribution_ChainDescription == "Pharmacies") | 
                           (Distribution_Channel_Name == "Pharmacies")
                       ) &
                       !is.na(Month)
)

dat <- dat[include.rows,] # Subset the data
```

## Brief data exploration

How many data points do we have, and what's the reporting period?
```{r}
length(unique(dat$Month))
range(dat$Month)
```

Here we inspect the Sellout_TurnoverBRG figures, aggregated by PHARMA_BRAND, for each month:
```{r}
dat %>% 
  arrange(Month) %>% 
  group_by(PHARMA_BRAND, Month) %>% 
  summarise(Total = sum(Sellout_TurnoverBRG)) %>% 
  ggplot(aes(x = Month, y = Total, colour = PHARMA_BRAND)) +
  geom_point() +
  geom_line()
```

Here we look at the Sellout_TurnoverBRG aggregated by PHARMA_GROUP (there's only two in the Golden Standard data):
```{r}
dat %>% 
  arrange(Month) %>% 
  group_by(PHARMA_GROUP, Month) %>% 
  summarise(Total = sum(Sellout_TurnoverBRG)) %>% 
  ggplot(aes(x = Month, y = Total, colour = PHARMA_GROUP)) +
  geom_point() +
  geom_line()
```

## Forecasting with Prophet

Facebook is a very data-driven company. They recently made available their *Prophet* package for time series analysis and forecasting that is very user-friendly and requires little tweaking or tuning of parameters to get good results. I've not read the paper yet, so don't ask me how the magic works.

Here we aggregate Sellout_TurnoverBRG by PHARMA_GROUP, as done for the previous plot, but then we select just the Bayer data to model:

```{r}
require(prophet)
dat %>% 
  arrange(Month) %>% 
  group_by(PHARMA_GROUP, Month) %>% 
  summarise(Total = sum(Sellout_TurnoverBRG)) %>% 
  filter(PHARMA_GROUP == "BAYER CC") -> temp

df <- temp[c("Month", "Total")] # We only want these columns
names(df) <- c("ds","y") # Prepare the data for prophet


m <- prophet(df, interval.width = 0.95) # 95% confidence interval
# Prophet now runs and outputs some information...
```

Here we build a table to contain our future forecast for 12 months in the future:
```{r}
future <- make_future_dataframe(m, periods = 12, freq = "month")
```

Here we run the prediction for the future:
```{r}
forecast <- predict(m, future)
```

Now we plot the original data and confidence interval bands (we chose a 95% confidence interval), also a decomposition of the data into the trend and seasonal components:
```{r}
plot(m, forecast)
prophet_plot_components(m, forecast)
```

Well, that was fun.

## Forecasting with ARIMA

Here we try fitting a bunch of ARIMA models to the data; the best model is chosen automatically:
```{r}
require(forecast)
temp <- ts(df$y, frequency = 12, start = c(2014, 7))
fit <- auto.arima(temp, trace = TRUE)
summary(fit)
```

Here we forecast a year into the future:
```{r}
arima.forecast <- forecast(fit, h = 12)
```

Here we plot the forecast with 80% and 95% confidence intervals bands for the predictions:
```{r}
plot(arima.forecast)
arima.forecast # This will print the numeric values for the forecast
```

That looks nice.

## Model validation

How about trying to use the data to *postdict* historical data? We'd like to have confidence that the predictions for past data match what actually happened.

We define a *crystal ball* function that will run prophet, plot the results, and plot held-out test data as red points. We'll partition the data into two parts, data before and after 2017. The data after 2017 will be our test data. How well do the predictions (the blue 95% confidence interval) match the test data (the red points)?
```{r}
fb.crystal.ball <- function(in.data) {
  tbl <- in.data[[1]]
  
  # Grab the brand name to use a the plot title:
  title <- tbl %>%
    select(PHARMA_BRAND) %>%
    group_by(PHARMA_BRAND) %>%
    summarise() %>%
    as.character()
  
  # Prepare and partition the data into test and train datasets:
  df <- tbl[c("Month", "Total")]
  names(df) <- c("ds","y")
  in.train <- df$ds < "2017-01-01"
  df.train <- df[in.train,]
  df.test <- df[!in.train,]
  
  # Forecast:
  m <- prophet(df.train, interval.width = 0.95) # 95% confidence interval
  future <- make_future_dataframe(m, periods = sum(!in.train), freq = "month")
  forecast <- predict(m, future)
  
  # Make a nice plot:
  plot(m, forecast) +
    geom_point(data = df.test,
               aes(x = as.POSIXct(ds), y = y),
               colour = "red") + # The test points will be red
    ggtitle(title)
}
```

Now we run this function on every PHARMA_BRAND and postdict the data for 2017:
```{r message = FALSE}
require(purrr)
results <- dat %>% arrange(Month) %>% 
  group_by(PHARMA_BRAND, Month) %>% 
  summarise(Total = sum(Sellout_TurnoverBRG)) %>% 
  #filter(PHARMA_BRAND == "BEPANTHEN TOTAL") %>% # Uncomment for testing
  do(data = (.)) %>% 
  split(.$PHARMA_BRAND) %>% 
  lapply(function(x) x$data) %>% 
  map(~fb.crystal.ball(.))
print(results)
```

That was kinda fun. 

How about using ARIMA models instead of prophet? Here we define a new function for producing ARIMA models:
```{r}
arima.crystal.ball <- function(in.data) {
  tbl <- in.data[[1]]

  # Grab the brand name to use a the plot title:
  title <- tbl %>%
    select(PHARMA_BRAND) %>%
    group_by(PHARMA_BRAND) %>%
    summarise() %>%
    as.character()
  
  # Prepare and partition the data into test and train datasets:
  df <- tbl[c("Month", "Total")]
  names(df) <- c("ds","y")
  in.train <- df$ds < "2017-01-01"
  df.train <- df[in.train,]
  train.ts <- ts(df.train$y, frequency = 12, start = c(2014, 7))
  df.test <- df[!in.train,]
  test.ts <- ts(df.test$y, frequency = 12, start = c(2017, 1))
  
  # Forecast:
  fit <- auto.arima(train.ts, trace = FALSE)
  arima.forecast <- forecast(fit, h = sum(!in.train))
  
  # Grab the data points to add to the plot:
  train <- as.data.frame(time(train.ts))
  names(train) <- "x"
  train$y <- as.vector(train.ts)
  
  test <- as.data.frame(time(test.ts))
  names(test) <- "x"
  test$y <- as.vector(test.ts)
  
  autoplot(arima.forecast) +
    ylab("Sellout_TurnoverBRG") +
    geom_point(data = train, aes(x = x, y = y), colour = "black") +
    geom_point(data = test, aes(x = x, y = y), colour = "red") +
    ggtitle(title)
}
```

We run our new crystal ball function on each of the PHARMA_BRANDS:
```{r message = FALSE}
results <- dat %>% arrange(Month) %>% 
  group_by(PHARMA_BRAND, Month) %>% 
  summarise(Total = sum(Sellout_TurnoverBRG)) %>% 
  # filter(PHARMA_BRAND == "BEPANTHEN TOTAL") %>% # Uncomment for testing
  do(data = (.)) %>% 
  split(.$PHARMA_BRAND) %>% 
  lapply(function(x) x$data) %>% 
  map(~arima.crystal.ball(.))
print(results)
```

OK, so it looks like we might be able to do some useful predictive work with the data. Performance seems to be variable. Some results seem quite good, which is encouraging, given that we didn't do any tuning --- we just supplied the data and hoped for the best. We could probably do better if we had more data and spent more time refining models. Not bad for a few hours work!

